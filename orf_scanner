#!/usr/bin/env python

# GY170801

''' orf_scanner: robust CDS prediction '''


from __future__ import print_function  # Py2
from argparse import ArgumentParser, FileType, SUPPRESS
from collections import namedtuple
from itertools import product
import logging
from math import log10
from random import choice
import re
from signal import signal, SIGPIPE, SIG_DFL


# Globals #####################################################################


signal(SIGPIPE, SIG_DFL)  # Gracefully handle downstream PIPE closure

# translation pattern for reverse-complement
trans = ('ATUCGXNRYWSKMDVHB', 'TAAGXXNYRWSMKHBDV')
try:
    _acgt_comp = str.maketrans(*trans)
except AttributeError:
    from string import maketrans  # Py2
    _acgt_comp = maketrans(*trans)

# data structures
seqrecord = namedtuple('seqrecord', ['seqid', 'seq', 'quals'])
orfrecord = namedtuple('orfrecord', ['frame', 'start', 'end', 'seq'])


# Classes #####################################################################


class MultiReader(object):
    ''' A class for reading FASTA/Q records '''

    def __init__(self, inputstream):
        self._fileobj = inputstream
        self._buffer = ' '

    def __enter__(self):
        return self

    def __exit__(self, type, value, traceback):
        pass

    def __iter__(self):
        return self

    def __next__(self):
        while self._buffer and self._buffer[0] not in ('>', '@'):
            self._buffer = self._fileobj.readline()
        if not self._buffer:
            raise StopIteration
        if self._buffer[0] == ">":
            seqid = self._buffer[1:].strip()
            self._buffer = self._fileobj.readline()
            seqlines = []
            while self._buffer and self._buffer[0] not in ('>', '@'):
                seqlines.append(self._buffer.strip())
                self._buffer = self._fileobj.readline()
            return seqrecord(seqid, ''.join(seqlines).upper(), '')
        else:
            seqid = self._buffer[1:].strip()
            seq = self._fileobj.readline().strip().upper()
            self._fileobj.readline()
            quals = self._fileobj.readline().strip()
            self._buffer = self._fileobj.readline()
            return seqrecord(seqid, seq, quals)

    next = __next__  # Py2 compatibility


# Functions ###################################################################


def iterkmers(iterable, k, step=1):
    ''' Yield all possible kmers of length k from a memory-mapped iterable '''
    for i in range(0, len(iterable)-k+1, step):
        yield iterable[i:i+k]


def rcomp(record):
    ''' Reverse complement a seqrecord '''
    return seqrecord._replace(
        seq=record.seq[::-1].translate(_acgt_comp),
        quals=record.quals[::-1])


def find_orfs(record, pattern, longest_only=False, sense_only=False):
    ''' Search all frames (0 -> 5) for ORFs using the provided regex '''

    def _orf_generator(seq, offset):
        ''' Yield ORFs from a specified frame (0 -> 2) '''
        m = pattern.match(seq, offset)
        while m:
            yield orfrecord(m.start(1)%3, m.start(1), m.end(1), m.group(1))
            m = pattern.match(seq, m.end())

    orfs = []
    for offset in (0, 1, 2):
        orfs += [orf for orf in _orf_generator(record.seq, offset)]
    if not sense_only:
        record = rcomp(record)
        for offset in (0, 1, 2):
            orfs += [orfrecord(orf.frame+3, orf.start, orf.end, orf.seq)
                for orf in _orf_generator(record.seq, offset)]
    if orfs and longest_only:
        return [max(orfs, key=lambda x: x.end - x.start)]
    return orfs


def orf_len(record):
    ''' The length of an ORF '''
    return record.end - record.start


def score_frames(seq):
    ''' Score all frames of an ORF '''
    scores = [
        sum(hexamers.get(k, 0) for k in iterkmers(seq, 6, 3)),
        sum(hexamers.get(k, 0) for k in iterkmers(seq[1:]+seq[:1], 6, 3)),
        sum(hexamers.get(k, 0) for k in iterkmers(seq[2:]+seq[:2], 6, 3))]
    seq = rcomp(seqrecord('', seq, '')).seq
    scores += [
        sum(hexamers.get(k, 0) for k in iterkmers(seq, 6, 3)),
        sum(hexamers.get(k, 0) for k in iterkmers(seq[1:]+seq[:1], 6, 3)),
        sum(hexamers.get(k, 0) for k in iterkmers(seq[2:]+seq[:2], 6, 3))]
    return scores


def rand_name(k=16):
    ''' Random hex string of length `k` '''
    return ''.join(choice('abcdef0123456789') for i in range(k))


###############################################################################


if __name__ == '__main__':

    # Parse commandline args ##################################################

    parser = ArgumentParser(
        description='orf_scanner: robust CDS prediction',
        argument_default=SUPPRESS)
    parser.add_argument(
        'input', nargs='+', type=FileType('r'),
        help='FASTA/Q input file(s) (not compatible with streams)')
    parser.add_argument(
        '-m', '--model', nargs='*', type=FileType('r'),
        help='FASTA/Q file(s) of validated, in-frame, sense-oriented CDSes \
        for hexamer and base-frequency modeling (when supplied with no files, \
        the longest 1000 ORFs from the `input` are used)')
    parser.add_argument(
        '-l', '--min_length', const='300', nargs='?', type=int,
        help='Minimum nucleotide length of ORFs (default %(const)s)')
    parser.add_argument(
        '-s', '--stranded', action='store_true',
        help='Consider only ORFs from sense frames')
    parser.add_argument(
        '--gff3', action='store_true',
        help='Output GFF3 format instead of FASTA sequences')
    parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='Minimum length of ORFs to report')
    args = parser.parse_args()

    stranded = True if 'stranded' in args else False

    # Setup logging system ####################################################

    logger = logging.getLogger()
    logstream = logging.StreamHandler()
    logstream.setFormatter(logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s', '%y%m%d %H:%M:%S'))

    if 'verbose' in args:
        loglevel = logging.INFO
    else:
        loglevel = logging.ERROR

    logging.basicConfig(level=loglevel, handlers=[logstream])
    logger.info('Started orf_scanner')

    # Generics ################################################################

    orf_pattern = re.compile('(?:...)*?(ATG(?:(?!TAA|TGA|TAG)...){' +
        '{}'.format(args.min_length//3-1) + ',}(?:TAA|TGA|TAG|..$|.$))')

    # Build hexamer and nucleotide models #####################################

    if 'model' in args:

        hexamers = {''.join(mer):0 for mer in product('ATCG', repeat=6)}
        bases = {'A': 0, 'T': 0, 'C': 0, 'G': 0}

        if args.model:  # use validated CDS regions
            logger.info('Using ORF models provided')
            for f in args.model:
                logger.info('   {}'.format(f.name))
                with MultiReader(f) as F:
                    for i, record in enumerate(F, 1):
                        for kmer in iterkmers(record.seq, k=6, step=3):
                            try:
                                hexamers[kmer] += 1
                            except KeyError:
                                pass
                        for k in bases.keys():
                            bases[k] += record.seq.count(k)
                logger.info('   {} sequences read'.format(i))

        else:  # use the 1000 longest ORFs
            logger.info('No models provided, finding 1000 longest ORFs')
            longest_orfs = []
            for f in args.input:
                logger.info('... {}'.format(f.name))
                with MultiReader(f) as F:
                    for i, record in enumerate(F, 1):
                        orf = find_orfs(record, orf_pattern, True, stranded)
                        if orf and longest_orfs:
                            for j, item in enumerate(longest_orfs):
                                if orf_len(orf[0]) > orf_len(item):
                                    break
                            longest_orfs[j:j] = orf
                            if len(longest_orfs) > 1000:
                                longest_orfs = longest_orfs[1:]
                        elif orf:
                            longest_orfs += orf
                    logger.info('   {} sequences read'.format(i))
            logger.info('   {} longest ORFs found'.format(len(longest_orfs)))
            for orf in longest_orfs:
                for kmer in iterkmers(orf.seq, k=6, step=3):
                    try:
                        hexamers[kmer] += 1
                    except KeyError:
                        pass
                for k in bases.keys():
                    bases[k] += orf.seq.count(k)

        logger.info('Calculating base and hexamer frequency models')
        bases = {k:v/sum(bases.values()) for k, v in bases.items()}
        if any(v == 0 for v in hexamers.values()):
            for k in hexamers.keys():
                hexamers[k] += 1
        hexamers = {k:log10(v/sum(hexamers.values())/sum(bases[c] for c in k))
            for k, v in hexamers.items()}

    # Identify ORFs ###########################################################

    logger.info('Identifying and scoring ORFs')
    retained, discarded = 0, 0
    for f in args.input:
        logger.info('   {}'.format(f.name))
        f.seek(0)
        with MultiReader(f) as F:
            for i, record in enumerate(F, 1):
                for j, orf in enumerate(
                        find_orfs(record, orf_pattern, False, stranded), 1):
                    if 'model' in args:
                        frame_scores = score_frames(orf.seq)
                        if frame_scores[0] >= max(frame_scores[1:]):
                            discarded += 1
                            continue
                    start, end = orf.start, orf.end
                    if orf.frame in (3, 4, 5):
                        frame = '-'
                        start, end = len(record.seq)-end, len(record.seq)-start
                    else:
                        frame = '+'
                    start += 1
                    if 'gff3' in args:
                        print('{}\torf_scanner\tprotein\t{}\t{}\t.\t{}\t0\t{}'
                            .format(record.seqid.split(' ')[0], start, end,
                                frame, 'ID=prot{}'.format(rand_name())))
                    else:
                        print('>{}_orf{}_{}_{}_{}\n{}'.format(
                            record.seqid.split(' ')[0], j, start, end,
                                frame, orf.seq))
                    retained += 1
        logger.info('   {} sequences read'.format(i))
    logger.info('{} potential ORFs found'.format(retained))
    if 'model' in args:
        logger.info('{} potential ORFs discarded'.format(discarded))
